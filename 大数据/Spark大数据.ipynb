{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.简述Hadoop的优点有哪些？Spark与之相比又有哪些优点？\n",
    "\n",
    "hadoop是一个适合大数据的分布式存储和计算的平台。优点：\n",
    "\n",
    "- 低成本: hadoop本身是运行在普通PC服务器组成的集群中进行大数据的分发及处理工作的，这些服务器集群是可以支持数千个节点的。\n",
    "- 高效性: 这也是hadoop的核心竞争优势所在，接受到客户的数据请求后，hadoop可以在数据所在的集群节点上并发处理。\n",
    "- 可靠性: 通过分布式存储，hadoop可以自动存储多份副本，当数据处理请求失败后，会自动重新部署计算任务。\n",
    "- 扩展性: hadoop的分布式存储和分布式计算是在集群节点完成的，这也决定了hadoop可以扩展至更多的集群节点。\n",
    "- 容错性: hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配\n",
    "\n",
    "Hadoop的局限性和不足：\n",
    "\n",
    "- 抽象层次低，需要手动编写mapper和reducer逻辑，使用上复杂\n",
    "- 只提供Map和reduce两个操作，表达力欠缺\n",
    "- 处理逻辑隐藏在代码细节中，没有整理逻辑\n",
    "- 中间结果也存放在HDFS上，IO和通信开销大\n",
    "- 时延高，只适用batch数据处理，对于交互式数据处理和实时数据处理的支持不够\n",
    "- 对于迭代式数据处理性能比较差 \n",
    "\n",
    "Spark相比hadoop的优点有：\n",
    "\n",
    "- 丰富的API，而后者只有map和reduce。\n",
    "- 中间结果不存磁盘，而后者需要把中间结果写磁盘。\n",
    "- 线程池模型减少task启动开销，而后者任务调度和启动开销很大。\n",
    "- 能充分利用内存速度快优势，减少IO操作，而后者不能。\n",
    "- 可以避免排序操作，而hadoop中map和reduce都需要排序。\n",
    "- 适合迭代计算（机器学习算法），而后者不适合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.请简述MapReduce这一编程模型的理解\n",
    "\n",
    "Hadoop包括分布式文件系统HDFS，分布式计算框架MapReduce。MapReduce是用于数据处理的一种编程模型，是hadoop的核心组件之一，可以通过mapreduce很容易在hadoop平台上进行分布式的计算编程。\n",
    "\n",
    "MapRedeuce其处理过程主要分为两个步骤：\n",
    "\n",
    "1、映射(Mapping)函数以Key/Value数据对作为输入，将输入数据经过业务逻辑计算产生若干仍旧以Key/Value形式表达的中间数。 MapReduce计算框架会自动将中间结果中具有相同Key值的记录聚合在一起，并将数据传送给Reduce函数内定义好的处理逻辑作为其输入值。\n",
    "\n",
    "2、聚合(Reducing)函数接收到Map阶段传过来的某个Key值及其对应的若干Value值等中间数据，函数逻辑对这个Key对应的Value内容进行处理，一般是对其进行累加、过滤、转换等操作，生成Key/Value形式的结果，这就是最终的业务计算结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.请简述RDD的含义，并写出针对RDD的两类操作（transformation与action)，每类下至少三种的操作。\n",
    "\n",
    "##### RDD定义：\n",
    "\n",
    "RDD(Resilient Distributed Datasets)，弹性分布式数据集是一个容错的、可以被并行操作的元素集合弹性分布数据集。是Spark的核心，也是整个Spark的架构基础。Spark是以RDD概念为中心运行的。\n",
    "\n",
    "RDD的一大特性是分布式存储，分布式存储在最大的好处是可以让数据在不同工作节点并行存储，以便在需要数据时并行运算。弹性指其在节点存储时，既可以使用内存，也可已使用外存，为使用者进行大数据处理提供方便\n",
    "\n",
    "##### 特性如下：\n",
    "\n",
    "- 是不变的数据结构存储\n",
    "- 只读特性，维护DAG以便通过重新计算获得容错性\n",
    "- 支持跨集群的分布式数据结构\n",
    "- 可以根据数据记录的key对结构进行分区\n",
    "- 提供了粗粒度的操作，且这些操作都支持分区\n",
    "- 将数据存储在内存中，从而提供了低延迟性\n",
    "\n",
    "##### 常用的transformation操作：\n",
    "\n",
    "<font color='red'>map(func)</font>：对调用map的RDD数据集中的每个element都使用func，然后返回一个新的RDD，这个返回的数据集是分布式的数据集\n",
    "\n",
    "<font color='red'>filter(func)</font>：对调用filter的RDD数据集中的每个元素都使用func，然后返回一个包含使func为true的元素构成的RDD\n",
    "\n",
    "<font color='red'>flatMap(func)</font>：和map差不多，但是flatMap生成的是多个结果,返回值是一个Seq(一个List)\n",
    "\n",
    "<font color='red'>sample(withReplacement, fraction, seed)</font>：从RDD中的item中采样一部分出来，有放回或者无放回\n",
    "\n",
    "<font color='red'>union(otherDataset)</font>：返回一个新的dataset，包含源dataset和给定dataset的元素的集合\n",
    "\n",
    "<font color='red'>distinct([numTasks]))</font>：对RDD中的item去重\n",
    "\n",
    "<font color='red'>groupByKey([numTasks])</font>：返回(K,Seq[V])，也就是hadoop中reduce函数接受的key-valuelist\n",
    "\n",
    "<font color='red'>reduceByKey(func, [numTasks])</font>：就是用一个给定的reduce func再作用在groupByKey产生的(K,Seq[V]),比如求和，求平均数\n",
    "\n",
    "<font color='red'>sortByKey([ascending], [numTasks])</font>：按照key来进行排序，是升序还是降序，ascending是boolean类型\n",
    "\n",
    "<font color='red'>join(otherDataset, [numTasks])</font>：当有两个KV的dataset(K,V)和(K,W)，返回的是(K,(V,W))的dataset,numTasks为并发的任务数\n",
    "\n",
    "<font color='red'>cartesian(otherDataset)</font>：笛卡尔积就是m*n\n",
    "\n",
    "<font color='red'>intersection(otherDataset)</font>：交集\n",
    "\n",
    "<font color='red'>substract(otherDataset)</font>：差集\n",
    "\n",
    "<font color='red'>sortBy(keyfunc, ascending=True, numPartitions=None)</font>：Sorts this RDD by the given keyfunc\n",
    "\n",
    "##### 常用的action操作：\n",
    "\n",
    "<font color='red'>reduce(func)</font>: 对RDD中的items做聚合\n",
    "\n",
    "<font color='red'>collect()</font>: 计算所有的items并返回所有的结果到driver端，接着 collect()会以Python list的形式返回结果\n",
    "\n",
    "<font color='red'>count()</font>: 返回的是dataset中的element的个数\n",
    "\n",
    "<font color='red'>first()</font>: 和上面是类似的，不过只返回第1个item\n",
    "\n",
    "<font color='red'>take(n)</font>: 类似，但是返回n个item\n",
    "\n",
    "<font color='red'>top(n)</font>: 返回头n个items，按照自然结果排序\n",
    "\n",
    "<font color='red'>countByKey()</font>: 返回的是key对应的个数的一个map，作用于一个RDD\n",
    "\n",
    "<font color='red'>foreach()</font>: 对dataset中的每个元素都使用func\n",
    "\n",
    "<font color='red'>takeSample()</font>: 指定采样个数，返回相应的数目\n",
    "\n",
    "<font color='red'>saveAsTextFile(path)</font>: 把dataset写到一个text file中，或者hdfs，或者hdfs支持的文件系统中，spark把每条记录都转换为一行记录，然后写到file中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Spark内置了机器学习库mllib，请写出使用该库完成一项机器学习任务的通用步骤¶\n",
    "\n",
    "##### 1、数据探索（EDA）\n",
    "\n",
    "首先，对数据、对需求或机器学习的目标进行分析，尤其对数据进行一些必要的探索，如了解数据的大致结构、数据量、各特征的统计信息、整个数据质量情况、数据的分布情况等。为了更好体现数据分布情况，数据可视化是一个不错方法。\n",
    "\n",
    "##### 2、预处理数据\n",
    "\n",
    "通过对数据探索后，可能发现不少问题：如存在缺失数据、数据不规范、数据分布不均衡、存在奇异数据、有很多非数值数据、存在很多无关或不重要的数据等等。这些问题的存在直接影响数据质量，为此，数据预处理工作应该就是接下来的重点工作，数据预处理是机器学习过程中必不可少的重要步骤，特别是在生产环境中的机器学习，数据往往是原始、未加工和处理过，数据预处理常常占据整个机器学习过程的大部分时间。数据预处理过程中，一般包括数据清理、数据转换、规范数据、特征选择等等工作。\n",
    "\n",
    "##### 3、训练模型\n",
    "\n",
    "在模型选择时，一般不存在某种对任何情况都表现很好的算法（这种现象又称为没有免费的午餐）。因此在实际选择时，一般会选用几种不同方法来训练模型，然后比较它们的性能，从中选择最优的这个。当然，在比较不同模型之前，我们需要先确认衡量性能的指标，对分类问题常用的是准确率或ROC曲线，对回归连续性目标值问题一般采用误差来评估。训练模型前，一般会把数据集分为训练集和测试集，或对训练集再细分为训练集和验证集，从而对模型的泛化能力进行评估。\n",
    "\n",
    "##### 4、评估模型与优化模型\n",
    "\n",
    "使用训练数据构建模型后，通常使用测试数据对模型进行测试，测试模型对新数据的测试。如果我们对模型的测试结果满意，就可以用此模型对以后的进行预测；如果我们测试结果不满意，我们可以优化模型，优化的方法很多，其中网格搜索参数是一种有效方法，当然我们也可以采用手工调节参数等方法。如果出现过拟合，尤其是回归类问题，我们可以考虑正则化方法来降低模型的泛化误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习\n",
    "\n",
    "    - 新手的Spark指南：https://www.jianshu.com/p/a3bb01cc4622\n",
    "    - Spark工作原理及核心RDD 详解：https://blog.csdn.net/u010330043/article/details/52293212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
