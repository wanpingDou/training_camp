### K-Means

##### 过程

1）数据预处理。归一化、离群点分析等。

2）随机选取$K$个聚类中心，$\mu^{(0)}_1,\mu^{(0)}_2,\mu^{(0)}_3,\ldots,\mu^{(0)}_k$

3)  构建损失函数：$J(c,\mu) = min_{\mu}min_{c}\sum_{i=1}^{n}(\parallel x_i-\mu_{ci} \parallel^{2}$

4) 重复迭代过程，迭代次数记为$t$。

 - 将每个样本$x_{i}$分到距离最近的簇$c_{i}^{t}$。
 - 遍历完样本重新计算聚类中心$\mu_{k}^{t+1}$。

##### 缺点

- 受离群点以及初始值使得每次划分结果不稳定、通常不是全局最优。
- 无法解决分布簇差别较大等（一类是另一类的好多倍）。
- 不太适合分类特征等（计算的距离）。

##### 优点

- 大数据算法可伸缩、高效。
- 时间复杂度$O(NKt)$，样本数$N$、聚类数$K$与迭代次数$t$的线性。
- 尽管达到局部最优，但通常可满足聚类需求。

##### 调优

- 数据归一化和离群点处理。
- 合理聚类数$K$。
  - 手肘法——计算不同$K$下的损失。（人工）
  - Gap Statistic方法——随机样本损失与实际样本损失之差。（自动化）
- 采用核函数。
  - 欧式距离度量，本质上假设了各数据簇的数据具有一样的先验概率，并呈现出球形或者高维球形分布，这种分布在实际种不常见。
  - 针对非凸数据分布形状，引入核函数，非线性映射增加数据线性可分的概率。



###### 备注：

Gap statistic method可以运用到任何聚类算法里面。该方法先比较不同k值聚类结果中组内变异量的总和（total within intracluster variation）。利用统计学的假设检验来比较TSS值与那些随机分布的参考数据集之间是否显著差异。蒙特卡罗算法生成均匀分布的数据集，数据集中的值属于[$min(x_i)$, $max(x_i)$]。Gap statistic计算如下:

假设我们聚类为$K$类，$C_r$表示第$r$个cluster，$n_r$表示$C_r$中元素的个数。那$C_r$中元素两两距离之和为 $$ D_r = \sum_{i,i'\in C_r}d_{ii'} $$ ，$i$和$i'$都是属于$Cr$中的元素，$d_{ii'}$的计算可以按照欧拉距离来进行计算。

$W_k$的计算方法如下： 
$$
W_k = \sum_{r=1}^{k}\frac{1}{2n_r} D_r
$$
$W_k$称为组内变异量的总和，参考数据与实际数据之间的Gap值 :
$$
Gap_n(k)\;=\;E_{n}(log(W_k)) - log(W_k) 
$$
这里的$E_{n}$表示含有n个样品的参考数据集的期望值，计算过程是采用自助法（bootstrapping）。在零假设的条件下，Gap值可以衡量其与参考数据偏离程度。从不同的k值中选择Gap值最大的k值，记为$\hat k$，这时的聚类结果与均匀分布的参考数据集相差最大，可以选为最佳聚类数目。 为了能够计算标记误（standard error）$s_k$，需要先计算参考数据集的$log(W_k)$的标准差$sd_k$： $$ s_k = sd_k \times \sqrt{1+\frac{1}{B}} $$ 最后选择最佳聚类数目可以用一个更加鲁棒性的过程来表示，也就选择一个最小的k值，满足： $$ Gap(k) \ge Gap(k+1) - s_{k+1} $$

计算过程：

​	1）根据不同的k值对实际数据进行聚类并计算$W_k$。

​	2）产生B个参考数据集（bootstrap法），按照不同的k值进行聚类，并计算Gap值：
$$
Gap(k) = \frac{1}{B}\sum_{b=1}^{B}log(W_k) - log(W_k)
$$
​	3）让$\bar{w} = (\frac{1}{B}\sum_{b}log(W_{kb}))$，标准差$sd(k) = \sqrt{\frac{1}{B}\sum_b(log(W_{kb})-\bar{w})^2}$，标准误$sk _= sd_k \times \sqrt{1+\frac{1}{B}}$ 。

​	4）选择满足$Gap(k) \ge Gap(k+1) - s_{k+1}$的最小k值。



