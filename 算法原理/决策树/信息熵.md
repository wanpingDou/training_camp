### 信息论（Information Theory）

香农最早发明熵。第一个事情：怎样衡量信息量大小？

1. 可能跟这个事情发生的概率有关。（越容易出现的事情可能信息量越大）$H(X)  \propto\frac{1}{P(X)}$
2. 两件事的信息量$H(X_1,X_2)\propto H(X_1)+H(X_2)$。
3. $H(X)\geqslant 0$。

综合满足以上三点，构造出信息量：
$$
H(X)=log\frac{1}{p(X)}
$$

### 信息熵（Information Entropy）

$$
Entropy(X)=E_X[H(X)] =-\sum_{X}p(X)logp(X)
$$

注意：

- 这里$log$以2为底数。
- 信息熵衡量所有事件的平均值信息量（信息量与概率成反比）。
- 信息熵可以解释为不纯度？为什么？
  - 信息熵越大，平均信息量就越大，整体平均概率就越小，当整体各个类型别所占的比例差不多时，随机抓取一个，越是不能确定该样本的类别，也就是说整体分布越是混乱，即不纯。



