### 信息论（Information Theory）

香农最早发明熵。第一个事情：怎样衡量信息量大小？

1. 可能跟这个事情发生的概率有关。（越容易出现的事情可能信息量越大）$H(X)  \propto\frac{1}{P(X)}$
2. 两件事的信息量$H(X_1,X_2)\propto H(X_1)+H(X_2)$。
3. $H(X)\geqslant 0$。

综合满足以上三点，构造出信息量：
$$
H(X)=log\frac{1}{p(X)}
$$

### 信息熵（Information Entropy）

$$
Entropy(X)=E_X[H(X)] =-\sum_{X}p(X)logp(X)
$$

注意：

- 这里$log$以2为底数。
- 信息熵衡量所有事件的平均值信息量（信息量与概率成反比）。
- 信息熵可以解释为不纯度？为什么？
  - 信息熵越大，平均信息量就越大，整体平均概率就越小，当整体各个类型别所占的比例差不多时，随机抓取一个，越是不能确定该样本的类别，也就是说整体分布越是混乱，即不纯。



### 信息增益（Information Gain）

$$
Gain(A) = Entropy(A)-\sum_{A_{i}} W_{i}Entropy(A_{i})
$$

注意：

- $Entropy(A)$划分之前的信息熵。
- 本分将$A$划分成$A_{1},A_{2},\dots$不同的部分。
- 对每一个划分部分计算熵的加权和，即为划分之后的熵。
- 整体熵-划分熵表示，由于划分使得整体纯度提升了多少。
- 信息增益越大越好？
  - 信息增益分两部分，信息增益越大：整体熵一定情况下，划分加权信息熵越小。
  - 整体混乱，不纯度大，信息熵大，在此认为整体信息熵固定。
  - 那么，就希望划分之后各个部分都很纯，即划分加权信息熵越小。
  - 因此，信息增益越大也就越好。



