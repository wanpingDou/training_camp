[TOC]

## 卷积神经网络简介

卷积神经网络由一个或多个卷积层和顶端的全连通层（也可以使用1x1的卷积层作为最终的输出）组成一种前馈神经网络。一般的认为，卷积神经网络是由Yann LeCun大神在1989年提出的LeNet中首先被使用，但是由于当时的计算能力不够，并没有得到广泛的应用，到了1998年Yann LeCun及其合作者构建了更加完备的卷积神经网络LeNet-5并在手写数字的识别问题中取得成功，LeNet-5的成功使卷积神经网络的应用得到关注。LeNet-5沿用了LeCun (1989) 的学习策略并在原有设计中加入了池化层对输入特征进行筛选 。LeNet-5基本上定义了现代卷积神经网络的基本结构，其构筑中交替出现的卷积层-池化层被认为有效提取了输入图像的平移不变特征，使得对于特征的提取前进了一大步，所以我们一般的认为，Yann LeCun是卷积神经网络的创始人。

2006年后，随着深度学习理论的完善，尤其是 计算能力的提升和参数微调（fine-tuning）等技术的出现，卷积神经网络开始快速发展，在结构上不断加深，各类学习和优化理论得到引入，2012年的AlexNet、2014年的VGGNet、GoogLeNet 和2015年的ResNet,使得卷积神经网络几乎成为了深度学习中图像处理方面的标配。



## 为什么要用卷积神经网络

对于计算机视觉来说，每一个图像是由一个个像素点构成，每个像素点有三个通道，分别代表RGB三种颜色(不计算透明度)，我们以手写识别的数据你MNIST举例，每个图像的是一个长宽均为28，channel为1的单色图像，0-9十个数字类别。如果使用全连接的网络结构，即，网络中的神经与相邻层上的每个神经元均连接，那就意味着我们的网络有28 * 28 =784个神经元（RGB3色的话还要*3），hidden层如果使用了15个神经元，需要的参数个数(w和b)就有：28 * 28 * 15 * 10 + 15 + 10=117625个，这个数量级到现在为止也是一个很恐怖的数量级，一次反向传播计算量都是巨大的，这还展示一个单色的28像素大小的图片，如果我们使用更大的像素，计算量可想而知。



## 结构组成

上面说到传统的网络需要大量的参数，但是这些参数是否重复了呢，例如，我们识别一个人，只要看到他的眼睛，鼻子，嘴，还有脸基本上就知道这个人是谁了。

- 只是用这些**局部的特征**就能做做判断了，并不需要所有的特征。 
- 另外一点就是我们上面说的可以有效提取了输入图像的**平移不变特征**，就好像我们看到了这是个眼睛，这个眼镜在左边还是在右边他都是眼睛，这就是平移不变性。 
- <u>通过卷积的计算操作来提取图像局部的特征，每一层都会计算出一些局部特征，这些局部特征再汇总到下一层，这样一层一层的传递下去，特征由小变大，最后再通过这些局部的特征对图片进行处理</u>，这样大大提高了计算效率，也提高了准确度。



### 1. 卷积层

##### 1.1 卷积计算
> - 定义一个权重矩阵W（一般对于卷积来说，称作卷积的核kernel也有有人称做过滤器filter），这个权重矩阵的大小一般为`3 * 3` 或者`5 * 5`，但是在LeNet里面还用到了比较大的`7 * 7`，现在已经很少见了，因为根据经验的验证，3和5是最佳的大小。
> - 在输入矩阵上使用我们的权重矩阵进行滑动，每滑动一步，将所覆盖的值与矩阵对应的值相乘，并将结果求和并作为输出矩阵的一项，依次类推直到全部计算完成。

新矩阵的大小是如何计算的呢？
##### 1.2 卷积核大小f
> 卷积核的大小，用f来表示。

##### 1.3 边界填充Padding
> 经过计算后矩阵的大小改变了，要使矩阵大小不改变，可以先对矩阵做一个填充，将矩阵的周围全部再包围一层，这个矩阵就变成了`7*7`,上下左右各加1，相当于 `5+1+1=7` 这时，计算的结果还是 `5 * 5`的矩阵，保证了大小不变，这里的p=1。

##### 1.4 步长Stride
> 每次滑动的距离，就是步长这个参数。

##### 1.5 计算公式

> n为我们输入的矩阵的大小，$ \frac{n-f+2p}{s} +1 $ 向下取整。（**重要！！！**）

##### 1.7 卷积层
> 在每一个卷积层中我们都会设置多个核，<u>每个核代表着不同的特征，这些特征就是我们需要传递到下一层的输出，而我们训练的过程就是训练这些不同的核</u>。（**重要！！！**）



### 2. 激活函数
> 由于卷积的操作也是线性的，所以也需要进行激活，一般情况下，都会使用relu。



### 3. 池化层（pooling）
> - 减少卷积层之间的连接，降低运算复杂程度.
> - 池化层的操作很简单，就想相当于是合并，我们输入一个过滤器的大小，与卷积的操作一样，也是一步一步滑动，但是过滤器覆盖的区域进行合并，只保留一个值。
> - 合并的方式也有很多种，例如，最大池化maxpooling，平均池化avgpooling。
> - 池化层的输出大小公式也与卷积层一样，由于没有进行填充，所以p=0，可以简化为$ \frac{n-f}{s} +1 $ 。



### 4. dropout层

> Dropout是2014年 Hinton 提出防止过拟合而采用的trick，增强了模型的泛化能力。

Dropout（随机失活）是指在深度学习网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中丢弃，相当于从原始的网络中找到一个更瘦的网络，说的通俗一点，就是随机将一部分网络的传播掐断，听起来好像不靠谱，但是通过实际测试效果非常好。
有兴趣的可以去看一下原文[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/v15/srivastava14a.html)这里就不详细介绍了。



### 5. 全连接层

> 全链接层一般是作为最后的输出层使用，卷积的作用是提取图像的特征，最后的全连接层就是要通过这些特征来进行计算，输出我们所要的结果了，无论是分类，还是回归。

<u>我们的特征都是使用矩阵表示的，所以再传入全连接层之前还需要对特征进行压扁，将他这些特征变成一维的向量，如果要进行分类的话，就是用sofmax作为输出，如果要是回归的话就直接使用linear即可。</u>（**重要！！！**）