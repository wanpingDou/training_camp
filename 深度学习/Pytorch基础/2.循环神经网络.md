## Recurrent layers

### class torch.nn.RNN(* args, ** kwargs)[source]

将一个多层的 `Elman RNN`，激活函数为`tanh`或者`ReLU`，用于输入序列。

对输入序列中每个元素，`RNN`每层的计算公式为 $$ h_t=tanh(w_{ih}* x_t+b_{ih}+w_{hh}* h_{t-1}+b_{hh}) $$ $h_t$是时刻$t$的隐状态。 $x_t$是上一层时刻$t$的隐状态，或者是第一层在时刻$t$的输入。如果`nonlinearity='relu'`,那么将使用`relu`代替`tanh`作为激活函数。

##### 参数说明:

- input_size – 输入`x`的特征数量。
- hidden_size – 隐藏层的神经元数量，也就是隐藏层的特征数量。
- num_layers – RNN的层数，默认值是 1。
- nonlinearity – 指定非线性函数使用`tanh`还是`relu`。默认是`tanh`。
- bias – 如果是`False`，那么RNN层就不会使用偏置权重  $b_{ih}$ 和 $b_{hh}$ ，默认是`True`
- batch_first – 如果`True`的话，那么输入`Tensor`的shape应该是[batch_size, time_step, feature]，输出也是这样。
- dropout – 如果值非零，那么除了最后一层外，其它层的输出都会套上一个`dropout`层。
- bidirectional – 如果`True`，将会变成一个双向`RNN`，默认为`False`。

##### `RNN`的输入： **(input, h_0)** ——重点

- input (seq_len, batch, input_size): 保存输入序列特征的`tensor`。`input`可以是被填充的变长的序列。细节请看`torch.nn.utils.rnn.pack_padded_sequence()`

- h_0 (num_layers * num_directions, batch, hidden_size): 保存着初始隐状态的`tensor`

##### `RNN`的输出： (output, h_n)——重点

- output (seq_len, batch, hidden_size * num_directions): 保存着`RNN`最后一层的输出特征。如果输入是被填充过的序列，那么输出也是被填充的序列。num_directions根据是“否为双向”取值为1或2。
- h_n (num_layers * num_directions, batch, hidden_size): 保存着最后一个时刻隐状态。

##### `RNN`模型参数:

- weight_ih_l[k] ——第`k`层的 `input-hidden` 权重， 可学习，形状是`(input_size x hidden_size)`。
- weight_hh_l[k] —— 第`k`层的 `hidden-hidden` 权重， 可学习，形状是`(hidden_size x hidden_size)`
- bias_ih_l[k] —— 第`k`层的 `input-hidden` 偏置， 可学习，形状是`(hidden_size)`
- bias_hh_l[k] —— 第`k`层的 `hidden-hidden` 偏置， 可学习，形状是`(hidden_size)`

##### 示例：

```python
import torch
rnn = torch.nn.RNN(20,50,2)
input = torch.randn(100 , 32 , 20)
h_0 =torch.randn(2 , 32 , 50)
output,hn=rnn(input ,h_0) 
print(output.size(),hn.size())
```

```
输出：torch.Size([100, 32, 50]) torch.Size([2, 32, 50])
```

![2020-03-13_011301](img/2020-03-13_011301.png)